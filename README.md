# reto_de

* last_record.py: Obtains last record for each town, using the defined URL.
                  It must be executed once per hour.

* prec_temp_and_merge.py: Obtains mean for precipitations and temperature values, for each town. It considers the last two hours, if possible. This process can be executed whenever is needed. It uses data generated by last_record.py process.

* utilities: Contains functions to be used from different modules.

* config.py: Contains configurations; URL and HEADERS to be used.

## Structure

* configs: to include configs files
* data: to store the resulting CSV files.
  
        * data_municipio: contains data values for each town.
        * data_municipio_x_hora: contains last_record.py results
        * data_temp_prec_mean: contains CSV files with temperature and precipitations mean, for each town. (Obtained with prec_temp_and_merge.py)
        * data_merged: contains CSV files with merged data (temperature and precipitations mean merged with values from data_municipios) and current.csv file
* scripts: contains scripts

## Assumptions

### For last_record.py process

* response.content always is compressed
* The process must generate a csv file for each requirement
* It is executed once per hour

### For prec_temp_and_merge.py process

* It takes the values from the last two processes, if exist. If there is only one file, the values are taken from this unique file.

### For utilities.py

* The convention used to versioning the files is YYYYMMDD_HHMMSS
* UTC time is used.

## Questions

* ¿Qué mejoras propondrías a tu solución para siguientes versiones?

* Agregaría una carpeta "logs" para dejar los archivos de logs
* Configuraría un mecanismo de retry para cuando se obtiene response.status_code de error
* Modificaría el código para que last_record pueda ser ejecutado mas de una vez por hora
* usar poetry para el manejo de dependencias
* incluir unit testing

* Tu solución le ha gustado al equipo y deciden incorporar otros procesos, habrá nuevas personas colaborando contigo, ¿Qué aspectos o herramientas considerarías para escalar, organizar y automatizar tu solución?

* Propondría usar Spark (procesamiento en paralelo, en memoria, para big data) y diferentes servicios de AWS, como ser S3, CloudWatch, Athenas, entre otros.
* Propondría usar CI/CD.
* Propondría usar contenedores.
